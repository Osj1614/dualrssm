{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#from data import *\n",
    "#from net import *\n",
    "#from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.StudentT(3, 0.4, 2).mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6., 6., 6.],\n",
       "        [6., 6., 6., 6., 6.],\n",
       "        [6., 6., 6., 6., 6.],\n",
       "        [6., 6., 6., 6., 6.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(4, 5) * 3\n",
    "b = torch.ones(4, 5) * 2\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1\n",
    "+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FactorModel(FactorModel.Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorModel(\n",
       "  (market_gru): GRU(64, 128, batch_first=True)\n",
       "  (stock_gru): GRU(256, 128, batch_first=True)\n",
       "  (stock_prior_net): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       "  (stock_post_net): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       "  (market_prior_net): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       "  (market_post_net): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       "  (stock_aggr): AttentionStockAggr(\n",
       "    (enc_mlp): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=95, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (6): Identity()\n",
       "      )\n",
       "    )\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (factor_net): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       "  (beta_net): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(torch.distributions, \"Categorical\") is torch.distributions.Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000, 0.4000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.Categorical(torch.tensor([0.1, 0.2, 0.3, 0.4])).probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.predict(torch.randn(2, 12, 32, 94), torch.randn(2, 12, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.1324, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.predict(torch.randn(2, 12, 32, 94), torch.randn(2, 12, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_loss': tensor(6.5891, grad_fn=<MseLossBackward0>),\n",
       " 'kl_loss': tensor(5.8791, grad_fn=<DivBackward0>),\n",
       " 'loss': tensor(12.4682, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_scalars(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['pred_loss'].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]), torch.Size([2, 32, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "dloader = StockSeqDataLoader(StockSeqDataLoader.Config('../data_mynp.npz', rank_return_same_time=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1970,\n",
       " 1971,\n",
       " 1972,\n",
       " 1973,\n",
       " 1974,\n",
       " 1975,\n",
       " 1976,\n",
       " 1977,\n",
       " 1978,\n",
       " 1979,\n",
       " 1980,\n",
       " 1981,\n",
       " 1982,\n",
       " 1983,\n",
       " 1984,\n",
       " 1985,\n",
       " 1986,\n",
       " 1987,\n",
       " 1988,\n",
       " 1989,\n",
       " 1990,\n",
       " 1991,\n",
       " 1992,\n",
       " 1993,\n",
       " 1994,\n",
       " 1995,\n",
       " 1996,\n",
       " 1997,\n",
       " 1998,\n",
       " 1999,\n",
       " 2000,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2004,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2008,\n",
       " 2009,\n",
       " 2010,\n",
       " 2011,\n",
       " 2012,\n",
       " 2013,\n",
       " 2014,\n",
       " 2015]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(dt.year for dt in dloader.dates[dloader.valid_steps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object StockSeqDataLoader.get_split at 0x7f43dc1c0190>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloader.get_split(('2001-01-01', '2020-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs, rets = dloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 512, 94), (12, 512))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrs.shape, rets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, losses, outs = net.loss(torch.from_numpy(chrs[None]), torch.from_numpy(rets[None, ..., None]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
